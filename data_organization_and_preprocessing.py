# -*- coding: utf-8 -*-
"""Data_Organization_and_Preprocessing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1utBQHr_oPlxhJyBFJ8dyVenwWwg_nBRx
"""

# Import the needed modules
import numpy as np
import pandas as pd
import glob

# Mount google drive to access data
from google.colab import drive
drive.mount('/content/drive')

# Call in the needed observed and GFS data

!wget https://raw.githubusercontent.com/swnesbitt/ATMS-597-SP-2020/master/ATMS-597-SP-2020-Project-4/KCMI_daily.csv
!wget https://raw.githubusercontent.com/swnesbitt/ATMS-597-SP-2020/master/ATMS-597-SP-2020-Project-4/KCMI_hourly.csv

# Combine the file names for easy combination into one dataframe
path = '/content/drive/My Drive/Colab Notebooks/Project4/daily/bufkit'  # path to the data
all_files = glob.glob(path + "/*.gfs_kcmi.buf_daily.csv")

li = []

for filename in all_files:
    df = pd.read_csv(filename, index_col=None, header=0)
    li.append(df)

# Combine the files into one dataframe
daily_data = pd.concat(li, axis=0, ignore_index=True)

# Sort the data chronologically
daily_data_sorted = daily_data.sort_values(by='Unnamed: 0', axis=0)

# Remove NA values
daily_data_final1 = daily_data_sorted.dropna(axis=0)

# Rename date column and declare index column
daily_data_final2 = daily_data_final1.rename(columns={'Unnamed: 0':'Date'})
daily_data_final = daily_data_final2.set_index('Date')

# Subset training and test data
daily_data_final_training = daily_data_final.loc[:'2019-01-01']
print(daily_data_final_training)
daily_data_final_test = daily_data_final.loc['2019-01-01':'2020-01-01']
print(daily_data_final_test)

# Create CSVs of the organized data
daily_data_final_training.to_csv('gfs_daily_data_final_training.csv')
daily_data_final_test.to_csv('gfs_daily_data_final_test.csv')

# Combine the file names for easy combination into one dataframe
path = '/content/drive/My Drive/Colab Notebooks/Project4/prof/bufkit'  # path to the data
all_files = glob.glob(path + "/*.gfs_kcmi.buf_prof.csv")

li_1 = []

for filename in all_files:
    df = pd.read_csv(filename, index_col=None, header=0)
    li_1.append(df)

# Combine the files into one dataframe
prof_data = pd.concat(li_1, axis=0, ignore_index=True)

# Sort the data chronologically
prof_data_sorted = prof_data.sort_values(by='Unnamed: 0', axis=0)

# Reformat the pressure level data into new columns
prof_data_sorted[['925 DWPC', '850 DWPC', '700 DWPC', '500 DWPC', '250 DWPC', '100 DWPC']] = prof_data_sorted.DWPC.str.split(",", expand=True)
prof_data_sorted[['925 HGHT', '850 HGHT', '700 HGHT', '500 HGHT', '250 HGHT', '100 HGHT']] = prof_data_sorted.HGHT.str.split(",", expand=True)
prof_data_sorted[['925 TMPC', '850 TMPC', '700 TMPC', '500 TMPC', '250 TMPC', '100 TMPC']] = prof_data_sorted.TMPC.str.split(",", expand=True)
prof_data_sorted[['925 UWND', '850 UWND', '700 UWND', '500 UWND', '250 UWND', '100 UWND']] = prof_data_sorted.UWND.str.split(",", expand=True)
prof_data_sorted[['925 VWND', '850 VWND', '700 VWND', '500 VWND', '250 VWND', '100 VWND']] = prof_data_sorted.VWND.str.split(",", expand=True)

# Drop unnecessary columns
prof_data_sorted_extra = prof_data_sorted.drop(['100 DWPC', 'PRES', 'DWPC', 'HGHT', 'TMPC', 'UWND', 'VWND'],axis=1)

# Remove nan values
prof_data_sorted_final = prof_data_sorted_extra.dropna(axis=0)

# Remove brackets from pressure level values where necessary
prof_data_sorted_final['925 DWPC'] = prof_data_sorted_final['925 DWPC'].str[1:].astype(float)
prof_data_sorted_final['925 HGHT'] = prof_data_sorted_final['925 HGHT'].str[1:].astype(float)
prof_data_sorted_final['925 TMPC'] = prof_data_sorted_final['925 TMPC'].str[1:].astype(float)
prof_data_sorted_final['925 UWND'] = prof_data_sorted_final['925 UWND'].str[1:].astype(float)
prof_data_sorted_final['925 VWND'] = prof_data_sorted_final['925 VWND'].str[1:].astype(float)
prof_data_sorted_final['100 HGHT'] = prof_data_sorted_final['100 HGHT'].str.extract('(\d+)').astype(float)
prof_data_sorted_final['100 TMPC'] = prof_data_sorted_final['100 TMPC'].str.extract('(\d+)').astype(float)
prof_data_sorted_final['100 UWND'] = prof_data_sorted_final['100 UWND'].str.extract('(\d+)').astype(float)
prof_data_sorted_final['100 VWND'] = prof_data_sorted_final['100 VWND'].str.extract('(\d+)').astype(float)

# Rename date column and set index column
prof_data_sorted1 = prof_data_sorted_final.rename(columns={'Unnamed: 0':'Date'})
prof_data_sorted2 = prof_data_sorted1.set_index('Date')

# Subset the training and test data
prof_data_final_training = prof_data_sorted2.loc[:'2019-01-01']
print(prof_data_final_training)
prof_data_final_test = prof_data_sorted2.loc['2019-01-01':'2020-01-01']
print(prof_data_final_test)

# Create CSVs of the organized data
prof_data_final_training.to_csv('gfs_prof_data_final_training.csv')
prof_data_final_test.to_csv('gfs_prof_data_final_test.csv')

# Combine the file names for easy combination into one dataframe
path = '/content/drive/My Drive/Colab Notebooks/Project4/sfc/bufkit'  # path to the data
all_files = glob.glob(path + "/*.csv")

li_2 = []

for filename in all_files:
    df = pd.read_csv(filename, index_col=None, header=None)
    li_2.append(df)

# Combine the files into one dataframe
sfc_data = pd.concat(li_2, axis=1, ignore_index=True)

# Transpose the data so date is on the left
sfc_data_transposed = sfc_data.T

# Sort the data chronologically
sfc_data_sorted = sfc_data_transposed.sort_values(by=0, axis=0)

# Replace missing data with nan
sfc_data_sorted.replace('-9999', np.nan)

# Remove NA data
sfc_data_final = sfc_data_sorted.dropna(axis=0)

# Replace column names
sfc_data_final.columns = ['DATE', 'DWPC', 'HCLD', 'LCLD', 'MCLD', 'PRCP', 'PRES', 'TMPC', 'UWND', 'VWND', 'WSPD']

# Set index column
sfc_data_final1 = sfc_data_final.set_index('DATE')

# Separate the data in to training and test data
sfc_data_final_training = sfc_data_final1.loc[:'2019-01-01']
print(sfc_data_final_training)
sfc_data_final_test = sfc_data_final1.loc['2019-01-01':'2020-01-01']
print(sfc_data_final_test)

# Save the organized data to CSV files
sfc_data_final_training.to_csv('gfs_sfc_data_final_training.csv')
sfc_data_final_test.to_csv('gfs_sfc_data_final_test.csv')

# Read in the observed data
daily_obs = pd.read_csv('KCMI_daily.csv', header=4, skipfooter=7, error_bad_lines=False, engine='python')
hourly_obs = pd.read_csv('KCMI_hourly.csv')

# Drop precip column to be replaced later
daily_obs_drop = daily_obs.drop('Total Precip (in)', axis=1)

# Remove columns except precip, set index, and change index to datetime
hourly_precip = hourly_obs.drop(['tmpc','dwpc','mslp','wdir','wspd','skct','pr6h','doy','woy'],axis=1) 
hourly_precip.set_index('Timestamp')
hourly_precip.index = pd.to_datetime(hourly_precip['Timestamp'])

# Resample precip to daily data
daily_precip = hourly_precip.resample('d').sum()
daily_precip.columns = ['Daily Precip (mm)']

# Set dates to match daily data date range and add the daily precip data to the daily dataframe
daily_precip_red = daily_precip[(daily_precip.index>'2009-12-31') & (daily_precip.index<'2020-01-01')]
daily_obs_drop['Daily Precip (mm)'] = daily_precip_red['Daily Precip (mm)'].values
daily_obs_drop

# Orgainze the daily data and convert to degrees C and m/s
# Drop nan and missing data
daily_obs_drop = daily_obs.drop('Unnamed: 5', axis=1)
daily_obs_drop2 = daily_obs_drop.replace('M', np.nan)
daily_obs_new_nan = daily_obs_drop2.dropna(axis=0)
daily_obs_new = daily_obs_new_nan.set_index('Date')
daily_obs_new['Max Hourly Temp (F)'] = (daily_obs_new['Max Hourly Temp (F)'].astype(float) - 32)*(5/9.)
daily_obs_new['Min Hourly Temp (F)'] = (daily_obs_new['Min Hourly Temp (F)'].astype(float) - 32)*(5/9.)
daily_obs_new['Max Wind Speed (mph)']  = (daily_obs_new['Max Wind Speed (mph)'].astype(float)) * 0.44704
daily_obs_new.columns = ['Max Hourly Temp (C)', 'Min Hourly Temp (C)', 'Max Wind Speed (m/s)', 'Total Precip (mm)']

# Organize hourly data, drop nan values, and change negative precip. to 0
hourly_obs_new = hourly_obs.dropna(axis=0)
hourly_obs_new2 = hourly_obs_new.set_index('Timestamp')
hourly_obs_new3 = hourly_obs_new2.replace(-0.1, 0)

# Organize the observed data into training and test
hourly_obs_final_training = hourly_obs_new3.loc[:'2019-01-01']
print(hourly_obs_final_training)
hourly_obs_final_test = hourly_obs_new3.loc['2019-01-01':'2020-01-01']
print(hourly_obs_final_test)

daily_obs_final_training = daily_obs_new.loc[:'2018-12-31']
print(daily_obs_final_training)
daily_obs_final_test = daily_obs_new.loc['2019-01-01':'2020-01-01']
print(daily_obs_final_test)

# Save the observed data to csv files
daily_obs_final_training.to_csv('daily_obs_training.csv')
daily_obs_final_test.to_csv('daily_obs_test.csv')
hourly_obs_final_training.to_csv('hourly_obs_training.csv')
hourly_obs_final_test.to_csv('hourly_obs_test.csv')